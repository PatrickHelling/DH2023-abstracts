<?xml version="1.0" encoding="UTF-8"?><?xml-model href="http://www.tei-c.org/release/xml/tei/custom/schema/relaxng/tei_allPlus.rng" type="application/xml" schematypens="http://relaxng.org/ns/structure/1.0"?><?xml-model href="http://www.tei-c.org/release/xml/tei/custom/schema/relaxng/tei_allPlus.rng" type="application/xml" schematypens="http://purl.oclc.org/dsdl/schematron"?><TEI xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.tei-c.org/ns/1.0"><teiHeader>
        <fileDesc>
            <titleStmt>
                <title>imgs.ai. A Deep Visual Search Engine for Digital Art History</title>
                <author n="OffertFabianOFFERT_Fabian_imgs_ai__A_Deep_Visual_Search_Engine_for_Digit.xml"><persName n="OffertFabian">
                        <surname>Offert</surname>
                        <forename>Fabian</forename>
                    </persName><affiliation>University of California, Santa Barbara, United States of America</affiliation><email>offert@ucsb.edu</email></author><meeting><date from-iso="2023-07-10" to-iso="2023-07-14"/><title>Digital Humanities 2023. Collaboration as Opportunity</title><placeName>Graz</placeName></meeting>
                <author n="BellPeterOFFERT_Fabian_imgs_ai__A_Deep_Visual_Search_Engine_for_Digit.xml"><persName n="BellPeter">
                        <surname>Bell</surname>
                        <forename>Peter</forename>
                    </persName><affiliation>Philipps-Universität Marburg, Germany</affiliation><email>peter.bell@uni-marburg.de</email></author><meeting><date from-iso="2023-07-10" to-iso="2023-07-14"/><title>Digital Humanities 2023. Collaboration as Opportunity</title><placeName>Graz</placeName></meeting>
            </titleStmt>
            
            <publicationStmt><publisher><orgName ref="http://d-nb.info/gnd/1137284463">Zentrum für Informationsmodellierung
                    - Austrian Centre for Digital Humanities, Karl-Franzens-Universität
                    Graz</orgName></publisher><date when="2023">2023</date><pubPlace>Graz</pubPlace><availability><licence target="https://creativecommons.org/licenses/by/4.0">Creative Commons BY
                    4.0</licence></availability></publicationStmt><seriesStmt><title>Digital Humanities 2023: Book of Abstracts</title><editor><persName ref="https://orcid.org/0000-0002-9256-0958"><forename>Walter</forename><surname>Scholger</surname></persName><affiliation><orgName>University of Graz</orgName><placeName>Graz, Austria</placeName></affiliation><email>walter.scholger@uni-graz.at</email></editor><editor><persName ref="https://orcid.org/0000-0002-1726-1712"><forename>Georg</forename><surname>Vogeler</surname></persName><affiliation><orgName>University of Graz</orgName><placeName>Graz, Austria</placeName></affiliation><email>georg.vogeler@uni-graz.at</email></editor><editor><persName ref="https://orcid.org/0000-0002-3919-993X"><forename>Toma</forename><surname>Tasovac</surname></persName><affiliation><orgName>Belgrade Center for Digital Humanities</orgName><placeName>Belgrade, Serbia</placeName></affiliation><email>ttasovac@humanistika.org</email></editor><editor><persName ref="https://orcid.org/0000-0002-4593-059X"><forename>Anne</forename><surname>Baillot</surname></persName><affiliation><orgName>Le Mans Université</orgName><placeName>Le Mans, France</placeName></affiliation><email>anne.baillot@univ-lemans.fr</email></editor><respStmt><resp>Data Management</resp><persName ref="https://orcid.org/0009-0007-9019-8215"><forename>Elisabeth</forename><surname>Raunig</surname></persName></respStmt><respStmt><resp>Data Management</resp><persName ref="https://orcid.org/0000-0003-1438-3236"><forename>Martina</forename><surname>Scholger</surname></persName></respStmt><respStmt><resp>Data Management</resp><persName ref="https://orcid.org/0000-0001-9116-0402"><forename>Elisabeth</forename><surname>Steiner</surname></persName></respStmt><respStmt><resp>Data Curation</resp><persName><forename>Johanna</forename><surname>Ofenauer</surname></persName></respStmt><respStmt><resp>Data Curation</resp><persName><forename>Christina</forename><surname>Burgstaller</surname></persName></respStmt><idno type="DOI">10.5281/zenodo.7961822</idno></seriesStmt>
            <sourceDesc>
                <p>Converted from a Word document</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application ident="DHCONVALIDATOR" version="1.22">
                    <label>DHConvalidator</label>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc><abstract><p>We present a Web application that facilitates the deep visual search of image collections using contemporary machine learning. We discuss image retrieval as a combined computer vision/human-computer interaction problem, and propose that the standardization of feature extraction is one of the main problems that digital art history faces today.</p></abstract><langUsage><language ident="en">English</language></langUsage>
            <textClass>
                <keywords scheme="ConfTool" n="category">
                    <term>Paper</term>
                </keywords>
                <keywords scheme="ConfTool" n="subcategory">
                    <term>Short Presentation</term>
                </keywords>
                <keywords scheme="ConfTool" n="keywords">
                    <term>image retrieval</term>
                    <term>machine learning</term>
                    <term>computer vision</term>
                    <term>digital art history</term>
                </keywords>
                <keywords scheme="ConfTool" n="topics">
                    <term>artificial intelligence and machine learning</term>
                    <term>image processing and analysis</term>
                    <term>information retrieval and querying algorithms and methods</term>
                    <term>Interface design</term>
                    <term>development</term>
                    <term>and analysis</term>
                    <term>Art history</term>
                    
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader><text>
        <body>
            <p>Art history, as a discipline, is concerned with 
                <hi rend="italic">multiple</hi> images. A 
                <hi rend="italic">singular</hi> image can only become a historical entity in relation to its ‘neighbors’, to similar and dissimilar, related and unrelated original works. The significance of this comparative element has been emphasized already by Wölfflin (1917, see also Bruhn and Scholtz 2017) and has been epitomized by Aby Warburg’s (2010, see also Gombrich 1970, Didi-Huberman 2017) idiosyncratic method of tracing iconographic elements across history. Any singular image, in other words, leads to an 
                <hi rend="italic">image corpus</hi>. Institutional art historical image corpora commonly consist of images and related metadata but search operations often target metadata exclusively. This is a result of the historical importance of periodization, attribution, and localization in art history. Many art historical questions, however, specifically those related to both semantic (e.g. iconography) 
                <hi rend="italic">and</hi> syntactic (e.g. style) aspects of an image, are irreducible to metadata. Such questions, then, can only be operationalized either as visual queries, that is, in the form of sample images that possess a unique combination of visual properties representing the query, or as fuzzy textual queries, that is, free-form descriptions of visual properties.
            </p>
            <p>Imgs.ai, which has been in public beta since the fall of 2020, addresses this combined CV-HCI challenge of distant viewing (Arnold and Tilton, 2019, see also Wevers and Smits 2020) by means of deep visual search by providing a Web-based interface (fig. 1), and machine-learning based backend that allow the end user to both descriptively and visually search arbitrary image datasets, with multiple significant institutional datasets already indexed. It was the first publicly available digital art history application to implement a multimodal approach to deep visual search in early 2021. We first review the state of the art in three classes of deep visual search tools (toolkits, macro interfaces, and search interfaces) and then proceed to describe both the front- and backend aspects of imgs.ai.</p>
            <p>Imgs.ai is set up to index datasets based on four kinds of feature extractors. VGG19 (Simonyan et al. 2015) is a convolutional neural network with a focus on stylistic similarity. The pose feature extractor is built on top of a Keypoint R-CNN model with a ResNet-50-FPN backbone (He et al. 2017), which is especially useful for datasets of figurative works, as shown for instance by Impett and Moretti (2017). The “raw” feature extractor simply treats a (resized) image’s color data as its embedding, and thus allows searching for images that use similar pallets. Finally, the CLIP extractor uses the pre-trained model of the same name (Radford et al. 2021) and enables multimodal search.</p>
            <figure>
                <graphic n="1001" width="16.002cm" height="11.184819444444445cm" url="Pictures/a4edaae3b457f0ded5701def0e8227c2.png" rend="inline"/>
                <head>Figure 1: CLIP-based imgs.ai search in the Rijksmuseum collection. The toolbar allows to switch datasets or embeddings, add an image as a positive or negative example, or begin a new search.</head>
            </figure>
            <p>One surprising exemplary result facilitated by CLIP that we describe in this paper is the list of search results for the prompt “Las Meninas”, which references the famous 1656 painting by Diego Velásquez. The painting is famous, in particular, for its play on representation. Such metapictorial aspects, however, are rarely included in the metadata of a work. If we run a search for “Las Meninas” in the collection of the Museum of Modern Art, New York, the results show the conceptual depth that CLIP facilitates. Among them are two photographic works, Joel Meyerowitz’ 
                <hi rend="italic">Untitled </hi>from
                <hi rend="italic"> The French Portfolio</hi> (1980) and Robert Doisneau’s 
                <hi rend="italic">La Dame Indignée</hi> (1948). Both are explicit plays on representation and clearly pick up on the same themes as 
                <hi rend="italic">Las Meninas</hi>, especially the question of the gaze relation between people in, and people before the image, to use George Didi-Huberman’s term. Another result is Richard Hamilton’s 
                <hi rend="italic">Picasso's Meninas</hi> from 
                <hi rend="italic">Homage to Picasso </hi>(1973) which takes up the structure of the Velásquez original but fills it with figures from Picasso paintings. Here, imgs.ai, via CLIP, picks up on the compositional similarity of both works.
            </p>
            <p>We finally discuss how imgs.ai is just one of potentially many solutions to the productive use of feature extraction (see Zhang et al. 2018) in digital art history. Given the increasing footprint of machine learning models it seems counterproductive to extract features more than once. We thus argue that the standardization of extracted features is thus the next big challenge the digital art history community has to solve.</p>
        </body>
        <back>
            <div type="bibliogr">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl>
                        <author>Arnold, Taylor</author> / <author>Tilton, Lauren</author>  (2019): “Distant Viewing: Analyzing Large Visual Corpora”, in: Digital Scholarship in the Humanities.
                    </bibl>
                    <bibl>
                        <author>Bruhn, Matthias</author> / <author>Scholtz, Gerhard</author>  (2017): 
                        <hi rend="italic">Der vergleichende Blick</hi>. Berlin: Dietrich Reimer Verlag.
                    </bibl>
                    <bibl>
                        <author>Didi-Huberman, Georges</author> (2017): 
                        <hi rend="italic">The Surviving Image: Phantoms of Time and Time of Phantoms: Aby Warburg’s History of Art</hi>. Pennsylvania State University Press.
                    </bibl>
                    <bibl>
                        <author>Gombrich, Ernst H.</author> (1970): 
                        <hi rend="italic">Aby Warburg</hi>. Warburg Institute, University of London.
                    </bibl>
                    <bibl>
                        <author>He, Kaiming</author> / <author>Gkioxari, Georgia</author> / <author>Dollár, Piotr</author> / <author>Girshick, Ross B.</author> (2017): “Mask R-CNN.” ArXiv Preprint 1703.06870.
                    </bibl>
                    <bibl>
                        <author>Impett, Leonardo</author> / <author>Moretti, Franco</author>  (2017): “Totentanz. Operationalizing Aby Warburg’s Pathosformeln”, in: New Left Review 107.
                    </bibl>
                    <bibl>
                        <author>Radford, Alec</author> / <author>Kim, Jong Wook</author> / <author>Hallacy, Chris</author> / <author>Ramesh, Aditya</author> / <author>Goh, Gabriel</author> / <author>Agarwal, Sandhini</author> / <author>Sastry, Girish et al.</author> (2021): “Learning Transferable Visual Models from Natural Language Supervision”, in: International Conference on Machine Learning (ICML): 8748–63.
                    </bibl>
                    <bibl>
                        <author>Simonyan, Karen</author> / <author>Zisserman, Andrew</author>  (2015): “Very Deep Convolutional Networks for Large-Scale Image Recognition.” ArXiv Preprint 1409.1556.
                    </bibl>
                    <bibl>
                        <author>Warburg, Aby</author> (2010): “Mnemosyne Einleitung”, in: 
                        <hi rend="italic">Werke</hi>, ed. by Martin Treml / Sigrid Weigel / Perdita Ladwig. Frankfurt am Main: Suhrkamp.
                    </bibl>
                    <bibl>
                        <author>Wevers, Melvin</author> / <author>Smits, Thomas</author>  (2020): “The Visual Digital Turn: Using Neural Networks to Study Historical Images”, in: Digital Scholarship in the Humanities 35, 1: 194–207.
                    </bibl>
                    <bibl>
                        <author>Wölfflin, Heinrich</author> (1917): 
                        <hi rend="italic">Kunstgeschichtliche Grundbegriffe. Das Problem der Stilentwicklung in der neueren Kunst</hi>. München: Verlag Hugo Bruckmann.
                    </bibl>
                    <bibl>
                        <author>Zhang, Richard</author> / <author>Isola, Phillip</author> / <author>Efros, Alexei A.</author> / <author>Shechtman, Eli</author> / <author>Wang, Oliver</author>  (2018): “The Unreasonable Effectiveness of Deep Features as a Perceptual Metric”, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition: 586–95.
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text></TEI>